\glsresetall
\chapter{Research Objectives and Approach}
\label{chap:research_objectives_and_approach}

In this Chapter, the problem is approached in detail and the objectives for this research are presented. The problem definition, how we tackled it and our main difficulties and the objectives involved to provide a possible solution are presented in Section~\ref{sec:problem_definition}~-~\nameref{sec:problem_definition}. Also, a compilation of research questions are presented and evaluated with some reasoning about possible ways to answer them, later in Section~\ref{sec:research_questions}~-~\nameref{sec:research_questions}.

\section{\todo{Problem Definition (--rename?)}}
\label{sec:problem_definition}

Modern distributed services are large, complex, and increasingly built upon other similarly complex distributed services. Debugging microservice based systems is not an easy task to perform. It involves the collection, interpretation, and display of information concerning the interactions among concurrently executing processes operating in distributed machines. \nameref{subsec:distributed_tracing} data helps keeping an history of work performed by these systems.

End-to-end tracing captures the work-fow of causally-related activity (e.g., work done to process a request) within and among the components of a distributed system. As distributed systems grow in scale and complexity, such tracing is becoming a critical tool for management tasks like diagnosis and resource accounting. However, as systems grow, resulting tracing data from system execution is growing as well~\cite{Sambasivan2014}.

Tracing data growth raises some problems. This data is used by system operators to gain insight and observability of the distributed system, however, with this tendency to increase in quantity and complexity, it is becoming an overwhelming task for operators.

There are some tools that help handling tracing data, such as the ones presented in Subsection~\ref{subsec:distributed_tracing_tools}~-~\nameref{subsec:distributed_tracing_tools}, however, they only perform the job of collecting tracing data, present this information to the user in more human-readable formats and provide mere forms of querying this type of data. For this reason, manually managing these growing microservice architectures are becoming an outdated approach due to their incomportability.

To address this issue, there is a great need of improving tracing data processing and automate the task of tracing analysis. However, at that point, we did not have any tracing data at our disposal to start working, thus acquire tracing data from a distributed system was an urgency. Obtain tracing data for study is hard because it represents working from these systems and contain confidential information about them, however,\todo{through a NDA (Non-Disclosure Agreement) and the help of professor Jorge Cardoso,} representing Huawei, we were able to gain access to tracing data generated by the company.

This tracing data set was the starting point for this research. It is in \texttt{OpenTracing} format and was provided by Huawei. This data had been gathered from an experimental \texttt{OpenStack} cluster used by the company for testing purposes, and covered two days of operation. This data is addressed in detail in Section~\ref{sec:data_set}~-~\nameref{sec:data_set}.

After having access to tracing data, we have developed some prototype tools for data ingestion and setted up a distributed tracing tool. Zipkin was used as a distributed tracing tool to ingest tracing data provided by Huawei. The decision to use Zipkin instead of Jaeger, fell in the fact that it were much simpler due to lesser feature configuration. This was done with the purpose of gain a clear visualization about the data that were given. From this we decided to perform several meetings with the objective of defining a research direction and be able to propose a solution.

In these meetings, the elements of this research project gathered to debate ideas and define a set of questions to answer, taking into consideration the defined problem. Professor Jorge Cardoso, representing Huawei, was the client of the designed solution. The approach taken was to create a shared Kanban Board~\cite{Ikonen2011}, containing multiple lanes, to perform the of generation and refinement of prototype questions. This process involved having prototype question in the first lane, and move them through every lane reaching the last one, transforming a prototype question into a final research question. These research questions were built taking into consideration:

\begin{enumerate}
    \item Main needs felted by operators in normal day-to-day tasks, troubleshooting distributed systems;
    \item Most common issues presented in these systems;
    \item Variables involved when these issues appear;
    \item Relationship between these variables and the most common issues.
\end{enumerate}

In the next Section~\ref{sec:research_questions}~-~\nameref{sec:research_questions}, the process to generate the research questions is explained and the research questions, in their final state, are presented.

\section{Research Questions}
\label{sec:research_questions}

In this Section, we start by explaining the process to generate the research questions. In the end, these questions are defined and a possible approach for each one of them is discussed.

As mentioned in previous section, a Kanban Board was created with five lanes: ``Initial Prototype'', ``To Refine (1)'', ``Interesting'', ``To Refine (2)'' and ``Final Research Questions''. Throughout these lanes, questions were improved and filtered before reaching their final state. Prototype questions where:

\begin{enumerate}
    \item What is the neighbourhood of one service?
    \item Is there any problem (Which are the associated heuristics)?
    \item Is there any faults related to the system design/architecture?
    \item What is the root problem, when A, B, C services are slow?
    \item How are requests coming from the client?
    \item How endpoints orders distributions are done?
    \item What is the behaviour of the instances?
    \item What is the length of each queue in a service?
\end{enumerate}

These initial questions were to general, therefore they were passed through every lane defined before. This refinement leaded to the generation of final state questions. Final questions, with their corresponding description~\textbf{(D)} and a starting point for the expected work~\textbf{(W)} involved, are defined bellow:

\begin{enumerate}
    \item Does any service present a significant change in the number of incoming requests?
    \item Does any service present a significant change in the number of outgoing requests?
          \begin{itemize}
              \item[D.] The number of requests are the number of calls performed to a service. These metrics represent a very important measurement for service monitoring, because it measures the service usage in time.
              \item[W.] To obtain these metrics, one must generate the service dependency graph throughout defined time-frames and retrieve the number of connections between every node presented in the graph.
          \end{itemize}

    \item Does any service present a significant change in response time?
          \begin{itemize}
              \item[D.] Response time represents the amount of time needed to respond to a call. It is considered one of the most important measurements in systems because represents their performance.
              \item[W.] Get the response time for every span (difference between end and start time present in the structure).
          \end{itemize}

    \item Is there a problem related to the work-flow of one (or more) requests?
          \begin{itemize}
              \item[D.] Work-flow of one request represents the interaction path triggered throughout the system.
              \item[W.] Generate service dependency graph, retrieve work-flow paths presented in the graph and gather information about the number of unique paths and type variation.
          \end{itemize}

    \item How do requests are being handled by a specific service? (Identify services that are experiencing unreliability periods)
          \begin{itemize}
              \item[D.] In the end, requests have success or not. This is represented by a status code in \gls{http} or an exception in \gls{rpc}. Measure the ratio of these values can help identify unreliability periods in services.
              \item[W.] Gather status codes or exceptions from spans and generate a ratio of success and error.
          \end{itemize}

    \item Which services are the most popular in the system? (Number of established connections)
          \begin{itemize}
              \item[D.] Popularity of a service stands for the number of established connections. This measurement is important because a failure in a very popular service can compromise the entire system.
              \item[W.] Generate service dependency graph, and calculate the degree of each node. Services with higher degree are the most popular in the system.
          \end{itemize}

    \item Does any service present a significant change in the services it uses to fulfil requests?
          \begin{itemize}
              \item[D.] Services tend to communicate with a set of other services. These services do not change often, therefore, patterns in service communication can be observed. If these patterns are violated without service redeployment and networking changes, one might be facing a possible traffic redirection.
              \item[W.] Generate service dependency graph, and retrieve the set of services that each service communicates. Gathering these values in time, lead to a history of communication between services and, therefore, pattern recognition can be applied to detect strange variations.
          \end{itemize}

    \item Is there a problem related to the constitution of the system?
          \begin{itemize}
              \item[D.] Constitution in microservices architecture represent which services are presented in the system. The study of entries and exits of services in the overall system network can help identifying problems in system constitution.
              \item[W.] Generate service dependency graph in consecutive time-frames and retrieve the entry / exit of services. Variation analysis of this data can lead to detect constitution problems presented in distributed systems.
          \end{itemize}

    \item Do traces follow OpenTracing specification? (Structural quality testing)
          \begin{itemize}
              \item[D.] Structure quality is always an important factor when using some dataset to analyse a system. This question aims to perform a structural test of spans presented in tracing against the defined specification.
              \item[W.] Produce a structural schema based on the proposed open source tracing specification -- OpenTracing --, and check every span.
          \end{itemize}

    \item How is time coverage of tracing? (Coverability quality testing)
          \begin{itemize}
              \item[D.] Time coverage is an important aspect in tracing, because this measurement can pinpoint possible failures in system instrumentation.
              \item[W.] In tracing, child spans should cover almost the total duration of their parent span. To perform this test, a span tree for each trace must be assembled and times ratios of the durations must be extracted.
          \end{itemize}
\end{enumerate}

%colleagues that work directly in the field were contacted and the questions were exposed to them. In the end, the ten questions that were produced in the final lane represented right what are some of their needs.

After having generated these final state questions, an analysis report was performed in order to group them in similar fields of end-to-end tracing use cases~\cite{Sambasivan2014}. Table~\ref{table:final_state_question_groups} present the defined groups and the associated questions.

\begin{table}[H]
    \caption{Final state questions groups.}
    \label{table:final_state_question_groups}
    \centering
    \begin{tabularx}{\linewidth} {
        |>{\hsize=0.8\hsize}X|
        >{\hsize=1.2\hsize}X|}
        \cline{1-2}
        \textbf{Group}
         & \textbf{Question numbers}                                                                                                  \\ \hline \hline
        1. Anomaly detection
         & 1.  Does any service present a significant change in the number of incoming requests? \newline
        2.  Does any service present a significant change in the number of outgoing requests? \newline
        3.  Does any service present a significant change in response time?                                                           \\ \hline
        2. Steady state problems
         & 4.  Is there a problem related to the work-flow of one (or more) requests? \newline
        5.  How do requests are being handled by a specific service? (Identify services that are experiencing unreliability periods) \\ \hline
        3. Distributed resource profiling
         & 6. Which services are the most popular in the system? (Number of established connections) \newline
        7. Does any service present a significant change in the services it uses to fulfil requests? \newline
        8. Is there a problem related to the constitution of the system?                                                                \\ \hline
        4. Quality of tracing
         & 9. Do traces follow OpenTracing specification? (Structural quality testing); \newline
        10. How is time coverage of tracing? (Coverability quality testing).                                                         \\ \hline
    \end{tabularx}
\end{table}

\todo{Explain the assignments...}

\todo{CONTINUE FROM HERE!}

For each group the following general questions were composed:

\begin{itemize}
    \item Group 1 - Is there any anomalous service?
    \item Group 2 - What is the overall reliability of the service?
    \item Group 3 - Which service consumes more time when considering the entire set of requests?
    \item Group 4 - How can we measure the quality of tracing?
\end{itemize}

From this we decided to tackle two groups: 1. Anomaly detection and 4. Quality of tracing and therefore, the selected general questions were: 1. Is there any anomalous service? and 4. How can we measure the quality of tracing?

The first question can be reduced to finding anomalies in observations of service or system behaviour, namely metrics and morphology.
%The first question -- in practice --, relates with the assumption that there exists an anomalous service with off-the-trend observations in the measured data.
In particular, we considered three metrics: number of incoming service calls, outgoing service calls and average response time. Our proposed solution in Section~\ref{sec:proposed_solution} must have this into consideration -- extract these metrics from tracing data.

For the second question, there are multiple ways to analyse quality in tracing. We explored two directions, first performing a trace structure testing against the defined OpenTracing specification --structural testing --, to determine if the tracing data complies with all the predefined requirements. Secondly, coverage testing for tracing data to determine how much of the duration of Span is covered by its children -- time coverability testing. The first kind would be more valuable if the specification was stricter, however, changing the standard was not an option at the time as the data had external providence -- discussed in Section~\ref{sec:conclusions_and_future_work}.

Questions presented in the remaining groups were not studied further in this research project.
%there were difficulties analysing them with traces.


\todo{Loose frase from ?data set?}
The main goal was improving the reliability of the cluster by automating analysis and gain insight extracting data from traces.






These final questions










These final questions, with a slight reformulation, could be exported to high level of abstraction functional requirements of the monitoring tool that we want to develop. After having this questions, we decided to study the current state of art to check how things are done nowadays regarding this subject and we found that some tools perform the process of convert spans and traces to a graph, that represents the system at that current time interval, however they do not perform any kind of analysis and study over the span tree and the graph after that\cite{spans_analysis}.

Considering this, what we decided to do was to develop a simple prototype tool to test some state of the art tools. What we were able to achieve was to do the reconstruction of the graph, using our own data (this data was provided by Prof. Jorge Cardoso, representing an approximate two hour collection of spans and traces, about 400.000 spans, generated by one of their clusters). At this point, and since we have already held the hands-on of some tools at the moment, we were ready to start and think about the solution we need to build. Therefore, we decided to specify the solution, and considered to build a monitoring tool named by ourselves, \textit{Graphy}.

In a very briefly explanation, we want that \textit{Graphy} be able to calculate relevant metrics from the span trees and the generated graphs, and to work with this kind of metrics to perform the system analysis and answer the questions exposed above. To perform some of this work, it will be resourcing to machine learning algorithms that we will need to study in parallel with the implementation, as we cannot predict what we might encounter when retrieving the metrics at the time. The machine learning algorithms are to process the metrics and perform some deductions regarding the system behaviour over the time.












\checkoddpage
\ifthenelse{\boolean{oddpage}}
{ % Odd page
    \newpage
    \blankpage}
{ % Even page
}